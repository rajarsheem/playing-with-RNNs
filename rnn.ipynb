{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "from os import listdir\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from random import randint\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324\n",
      "735\n",
      "[29, 56, 8, 1, 82, 9, 45, 22, 24, 67]\n",
      "[56, 8, 1, 82, 9, 45, 22, 24, 67, 2]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 324\n",
    "d, id_to_token = data.get_data(vocab_size)\n",
    "for k in id_to_token:\n",
    "    if id_to_token[k] == 'eos':\n",
    "        id_to_token[k] = '\\n'\n",
    "print len(d)\n",
    "x, y = d[10:20], d[11:21]\n",
    "print [np.argmax(j) for j in x]\n",
    "print [np.argmax(j) for j in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dirrs = ['sentiment/train/', 'sentiment/test/']\n",
    "# sent = []\n",
    "# for dirr in dirrs:\n",
    "#     print dirr\n",
    "#     l = listdir(dirr+'pos')\n",
    "#     print 'pos'\n",
    "#     for r in l:\n",
    "#         t = codecs.open(dirr+'pos/'+r,'r',encoding='utf8').read()\n",
    "#         sent.append(nltk.word_tokenize(t))\n",
    "#     l = listdir(dirr+'neg')\n",
    "#     print 'neg'\n",
    "#     for r in l:\n",
    "#         t = codecs.open(dirr+'neg/'+r,'r',encoding='utf8').read()\n",
    "#         sent.append(nltk.word_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# m = Word2Vec.load('embedding.model')\n",
    "# data = []\n",
    "# for s in sent:\n",
    "#     a = [m[x] for x in s]\n",
    "#     data.append(a)\n",
    "# data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def sam(d):\n",
    "#     r = randint(0, data.shape[0] - 1)\n",
    "#     t = d[r]\n",
    "#     x = t[:-1]\n",
    "#     y = t[1:]\n",
    "#     return np.array(x), np.array(y)\n",
    "\n",
    "# s = sample(data)\n",
    "# s[0].shape, s[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "epochs = 60000\n",
    "hidden_layer = 128\n",
    "inp_out_size = 324\n",
    "learning_rate = 0.001\n",
    "num_steps = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.ops.variables.Variable object at 0x7fec9996bc90>, <tensorflow.python.ops.variables.Variable object at 0x7fec980baa10>, <tensorflow.python.ops.variables.Variable object at 0x7fec980ba9d0>, <tensorflow.python.ops.variables.Variable object at 0x7fec980d4610>, <tensorflow.python.ops.variables.Variable object at 0x7fec980e44d0>]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Wxh = tf.Variable([[.1, .2], [.3, .4], [.5, .6], [.7, .8]], dtype=tf.float32)\n",
    "# Whh = tf.Variable([[.1, .2], [.3, .4]], dtype=tf.float32)\n",
    "# Why = tf.Variable([[.1, .2, .3, .4], [.4, .5, .6, .7]], dtype=tf.float32)\n",
    "Wxh = tf.Variable(tf.random_normal([inp_out_size, hidden_layer], mean=0, stddev=0.001))\n",
    "Whh = tf.Variable(tf.random_normal([hidden_layer, hidden_layer], mean=0, stddev=0.001))\n",
    "Why = tf.Variable(tf.random_normal([hidden_layer, inp_out_size], mean=0, stddev=0.001))\n",
    "bh = tf.Variable(tf.zeros([hidden_layer]), dtype=tf.float32)\n",
    "by = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)\n",
    "print tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recurrence(prev, inp):\n",
    "    i = tf.reshape(inp, shape=[1, -1])\n",
    "    p = tf.reshape(prev, shape=[1, -1])\n",
    "    h = tf.tanh((tf.matmul(p, Whh))  + tf.matmul(i, Wxh) + bh)\n",
    "    h = tf.reshape(h, [hidden_layer])\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "b = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "initial = tf.placeholder(shape=[hidden_layer], dtype=tf.float32)\n",
    "states = tf.scan(recurrence, a, initializer=initial)\n",
    "outputs = tf.nn.softmax(tf.matmul(states, Why) + by)\n",
    "loss = -tf.reduce_sum(b*tf.log(outputs))\n",
    "# loss = tf.sqrt(tf.reduce_sum(tf.square(tf.sub(outputs, b))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "# clipping gradients between -1 and 1.\n",
    "grad_var_pairs = optimizer.compute_gradients(loss, tf.trainable_variables())\n",
    "clipped_grad_var_pairs = [(tf.clip_by_value(gv[0], -1, 1), gv[1]) for gv in grad_var_pairs]\n",
    "optimize_op = optimizer.apply_gradients(clipped_grad_var_pairs)\n",
    "\n",
    "# optimize_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(sess, n):\n",
    "    x, _ = data.sample(d, 1)\n",
    "    gen = [id_to_token[np.argmax(x[0])]]\n",
    "    h = np.zeros(hidden_layer)\n",
    "    for i in range(n):\n",
    "        o, h = sess.run([outputs, states], {a:x, initial: h})\n",
    "        h = h.reshape(hidden_layer)\n",
    "        o = np.argmax(o[0])\n",
    "        gen.append(id_to_token[o])\n",
    "        x = [0] * inp_out_size\n",
    "        x[o] = 1\n",
    "#         print np.argmax(x)\n",
    "        x = [x]\n",
    "    print ' '.join(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "ix = 0\n",
    "smooth_loss = -np.log(1.0 / vocab_size)*num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "epoch 0, loss = 74.1500637822\n",
      "epoch 1000, loss = 71.4036227919\n",
      "epoch 2000, loss = 68.7669468156\n",
      "epoch 3000, loss = 65.1413892697\n",
      "epoch 4000, loss = 62.8374848909\n",
      "epoch 5000, loss = 61.8369479722\n",
      "epoch 6000, loss = 61.3870267922\n",
      "epoch 7000, loss = 61.1182143732\n",
      "epoch 8000, loss = 60.8779046606\n",
      "epoch 9000, loss = 60.5381072472\n",
      "epoch 10000, loss = 60.0850656331\n",
      "epoch 11000, loss = 59.5457284149\n",
      "epoch 12000, loss = 58.9689993363\n",
      "epoch 13000, loss = 58.3150665894\n",
      "epoch 14000, loss = 57.5510557253\n",
      "epoch 15000, loss = 56.7005705248\n",
      "epoch 16000, loss = 55.6742156223\n",
      "epoch 17000, loss = 54.4605514208\n",
      "epoch 18000, loss = 53.0528639365\n",
      "epoch 19000, loss = 51.4728144186\n",
      "epoch 20000, loss = 49.6241414483\n",
      "epoch 21000, loss = 47.5471605026\n",
      "epoch 22000, loss = 45.4261219301\n",
      "epoch 23000, loss = 43.4088280672\n",
      "epoch 24000, loss = 41.4144884494\n",
      "epoch 25000, loss = 39.4337976094\n",
      "epoch 26000, loss = 37.4555986506\n",
      "epoch 27000, loss = 35.534961589\n",
      "epoch 28000, loss = 33.5491288219\n",
      "epoch 29000, loss = 31.5809222872\n",
      "epoch 30000, loss = 29.6275368533\n",
      "epoch 31000, loss = 27.7452832736\n",
      "epoch 32000, loss = 25.8397964234\n",
      "epoch 33000, loss = 24.0000406473\n",
      "epoch 34000, loss = 22.2437297696\n",
      "epoch 35000, loss = 20.6162408832\n",
      "epoch 36000, loss = 19.0065834389\n",
      "epoch 37000, loss = 17.5363084969\n",
      "epoch 38000, loss = 16.1678741382\n",
      "epoch 39000, loss = 14.921219285\n",
      "epoch 40000, loss = 13.7726452902\n",
      "epoch 41000, loss = 12.7172879421\n",
      "epoch 42000, loss = 11.7639502508\n",
      "epoch 43000, loss = 10.8801142075\n",
      "epoch 44000, loss = 10.1131878517\n",
      "epoch 45000, loss = 9.3951509259\n",
      "epoch 46000, loss = 8.75232575619\n",
      "epoch 47000, loss = 8.17943248641\n",
      "epoch 48000, loss = 7.66620691869\n",
      "epoch 49000, loss = 7.19909728276\n",
      "epoch 50000, loss = 6.77364581346\n",
      "epoch 51000, loss = 6.39277288677\n",
      "epoch 52000, loss = 6.05424082519\n",
      "epoch 53000, loss = 5.73928080964\n",
      "epoch 54000, loss = 5.44911976699\n",
      "epoch 55000, loss = 5.18842965499\n",
      "epoch 56000, loss = 4.95884871944\n",
      "epoch 57000, loss = 4.733844152\n",
      "epoch 58000, loss = 4.52979905314\n",
      "epoch 59000, loss = 4.34054843454\n"
     ]
    }
   ],
   "source": [
    "print epochs\n",
    "h = np.zeros(hidden_layer)\n",
    "for i in range(epochs):\n",
    "#     x, y = data.sample(d, num_steps)\n",
    "    if ix + num_steps >= len(d):\n",
    "        ix = 0\n",
    "    num_steps = num_steps - 1 if i % 2 == 0 else num_steps + 1 # variable length sequences\n",
    "    h = np.zeros(hidden_layer)\n",
    "    x, y = d[ix : ix + num_steps], d[ix + 1 : ix + num_steps + 1]   \n",
    "    l, h, _ = sess.run([loss, states, optimize_op], {a: x, b: y, initial: h}) \n",
    "    smooth_loss = smooth_loss * 0.999 + l * 0.001\n",
    "    if i % 1000 == 0:\n",
    "        print 'epoch {0}, loss = {1}'.format(i, smooth_loss)\n",
    "#         print sess.run(Whh)[0][:30]\n",
    "    ix += num_steps\n",
    "        \n",
    "#         generate(sess,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of Torment taught her \n",
      " How to frown and how to chide ; â€”thou \n",
      " Through 's winged Fancy roam , \n",
      " From with tasting 'd 'd : \n",
      " She has winged self-overaw every , \n",
      " Fades a meet in every mind : \n",
      " not \n",
      " gaz 's winged Fancy roam \n",
      " Doth \n",
      " Through 's winged Fancy roam , \n",
      " From with tasting 'd straw moment , \n",
      " Or will bring self-overaw 's mind \n",
      " Doth not hear in every new roam : \n",
      " not with tasting 'd straw stealth : \n",
      " She will gaz 'd 's Fancy ; \n",
      " Cloys , winged Fancy : \n",
      " She a winged self-overaw every , \n",
      " Fades will meet self-overaw every mind : \n",
      " not with gaz every straw The , \n",
      " Or a winged Fancy every muffled , \n",
      " Or will meet self-overaw every mind : \n",
      " \n",
      " much gaz 'd at home , \n",
      " \n",
      " not , tasting ever home : \n",
      " Or will bring self-overaw 's mind : \n",
      " Too much gaz 'd at stealth : \n",
      " She will bring self-overaw every mind : \n",
      " \n",
      " much gaz 'd at home . \n",
      " \n",
      " \n",
      " much 's winged Fancy home \n",
      " She has tasting clasp every at home \n",
      " Sweet \n",
      " golden 's winged Fancy home : \n",
      " The for tasting 'd 'd : \n",
      " She will meet self-overaw every , \n",
      " Doth not hear \n",
      " Distant clasp every self-overaw home : \n",
      " Too much gaz 'd at stealth : \n",
      " She will bring self-overaw every mind : \n",
      " \n",
      " much gaz 's at glance . , \n",
      " Too much gaz 'd and Fancy ; \n",
      " She will joys Fancy , and ! ; \n",
      " All delights joys are 's , and down not \n",
      " Sweet has tasting clasp , at Fancy ; \n",
      " Summer 's winged Fancy roam \n",
      " Doth not hear \n",
      " O clasp every self-overaw ever too : \n",
      " \n",
      " will of summer 'd roam , \n",
      " She will bring self-overaw 's mind \n",
      " Fades not hear in every self-overaw roam , \n",
      " From with tasting 'd straw moment , \n",
      " Or will gaz self-overaw 's mind \n",
      " One would hear behold Distant : \n",
      " She delights of summer Fancy roam \n",
      " One not hear behold winged clasp every , \n",
      " Fancy delights is summer \n",
      " In 's winged Fancy roam , \n",
      " From with tasting 'd straw moment , \n",
      " Or will bring self-overaw 's at ; \n",
      " Sweet with tasting 'd 's mind : \n",
      " Or much gaz 'd in spite : \n",
      " She will winged self-overaw 's , \n",
      " With will meet in every mind : \n",
      " She will bring self-overaw 'd , \n",
      " Fades not hear \n",
      " Distant clasp every in ever stealth \n",
      " \n",
      " \n",
      " much gaz and Fancy The bring the winged Fancy Distant the winged Fancy meet to winged \n",
      " Through the thought Fancy roam , \n",
      " From with tasting 'd ever new , \n",
      " Or will bring self-overaw every\n"
     ]
    }
   ],
   "source": [
    "generate(sess, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03073823,  0.16024236,  0.16885728, ...,  0.05288818,\n",
       "        -0.07688183, -0.03028169],\n",
       "       [-0.24785835, -0.04390815,  0.1450949 , ..., -0.05311445,\n",
       "        -0.01250314,  0.11281139],\n",
       "       [-0.33347312, -0.20284222,  0.23098749, ..., -0.10504574,\n",
       "        -0.28402022,  0.03956333],\n",
       "       ..., \n",
       "       [ 0.05662825,  0.16807128,  0.00710124, ...,  0.22454964,\n",
       "        -0.06467728,  0.10287257],\n",
       "       [-0.06466194, -0.10157995, -0.11470366, ...,  0.09235065,\n",
       "         0.3563292 ,  0.02229096],\n",
       "       [ 0.08362083,  0.13572359,  0.1389745 , ...,  0.22135787,\n",
       "         0.00691832,  0.11428044]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(Whh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
